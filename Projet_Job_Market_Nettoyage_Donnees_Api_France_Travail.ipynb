{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "f1324591",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Chunk de départ : 10000 documents\n",
      "Nettoyage des données: 10000 -> 9084 documents\n",
      "Documents stockés après ce chunk : 9084\n",
      "Chunk de départ : 10000 documents\n",
      "Nettoyage des données: 10000 -> 9039 documents\n",
      "Documents stockés après ce chunk : 18123\n",
      "Chunk de départ : 1233 documents\n",
      "Nettoyage des données: 1233 -> 1211 documents\n",
      "Documents stockés après ce chunk : 19334\n",
      "Nettoyage et sauvegarde des données terminés. Nombre total de documents transformés : 21233\n",
      "Nombre total de documents réellement stockés : 19334\n",
      "Nombre total de documents dans le fichier CSV : 19334\n"
     ]
    }
   ],
   "source": [
    "import ijson\n",
    "import pandas as pd\n",
    "import numpy as np\n",
    "import re\n",
    "import warnings\n",
    "warnings.filterwarnings('ignore', category=pd.errors.PerformanceWarning)\n",
    "\n",
    "# Chemin vers le fichier JSON volumineux dans Google Drive\n",
    "file_path = 'offres_emploi.json'\n",
    "\n",
    "# Lire le fichier JSON par morceaux\n",
    "def read_large_json(file, chunksize=10000):\n",
    "    \"\"\"\n",
    "    Cette fonction lit un fichier JSON volumineux par morceaux de taille spécifiée.\n",
    "\n",
    "    Args:\n",
    "    file (str): Le chemin du fichier JSON.\n",
    "    chunksize (int): Le nombre de lignes par morceau.\n",
    "\n",
    "    Yields:\n",
    "    list: Une liste d'objets JSON correspondant à un morceau du fichier.\n",
    "    \"\"\"\n",
    "    with open(file, 'r', encoding='utf-8') as f:\n",
    "        objects = ijson.items(f, 'item')\n",
    "        chunk = []\n",
    "        for i, obj in enumerate(objects):\n",
    "            chunk.append(obj)\n",
    "            if (i + 1) % chunksize == 0:\n",
    "                yield chunk\n",
    "                chunk = []\n",
    "        if chunk:\n",
    "            yield chunk\n",
    "\n",
    "# Fonction de nettoyage des données\n",
    "def clean_data(df):\n",
    "    \"\"\"\n",
    "    Cette fonction nettoie les données d'un DataFrame en supprimant les lignes avec des valeurs manquantes\n",
    "    dans certaines colonnes et en transformant certaines colonnes.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Le DataFrame à nettoyer.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Le DataFrame nettoyé.\n",
    "    \"\"\"\n",
    "    # Supprimer les lignes avec des valeurs manquantes dans des colonnes critiques\n",
    "    initial_count = len(df)\n",
    "    df.dropna(subset=['intitule', 'dateCreation', 'entreprise_nom'], inplace=True)\n",
    "    cleaned_count = len(df)\n",
    "\n",
    "#     # Remplacer les valeurs manquantes des colonnes numériques par 0\n",
    "#     numeric_columns = df.select_dtypes(include=['float64']).columns\n",
    "#     df[numeric_columns] = df[numeric_columns].fillna(0)\n",
    "\n",
    "    print(f\"Nettoyage des données: {initial_count} -> {cleaned_count} documents\")\n",
    "\n",
    "    # Fonction pour transformer la colonne dureeTravailLibelle\n",
    "    def transform_duree_travail_libelle(value):\n",
    "        if isinstance(value, str):\n",
    "            if \"temps partiel\" in value:\n",
    "                return 24\n",
    "            elif \"temps plein\" in value:\n",
    "                return 35\n",
    "            else:\n",
    "                numbers = re.findall(r'\\d+', value)\n",
    "                return int(numbers[0]) if numbers else 0\n",
    "        else:\n",
    "            return 0\n",
    "\n",
    "    # Appliquer la transformation à dureeTravailLibelle\n",
    "    df['dureeTravailLibelle'] = df['dureeTravailLibelle'].apply(transform_duree_travail_libelle)\n",
    "\n",
    "    # Calculer la moyenne de dureeTravailLibelle\n",
    "    duree_moyenne = df['dureeTravailLibelle'].mean()\n",
    "\n",
    "    # Remplacer les valeurs manquantes de dureeTravailLibelle par la moyenne\n",
    "    df['dureeTravailLibelle'].fillna(duree_moyenne, inplace=True)\n",
    "\n",
    "    # Fonction pour extraire le salaire en float\n",
    "    def extract_salaire(salaire, duree):\n",
    "        if isinstance(salaire, str) and 'Horaire' in salaire:\n",
    "            montants_list = re.findall(r'\\d+\\.\\d+', salaire)\n",
    "            montant = float(montants_list[0]) if montants_list else 0\n",
    "            heures_travail = duree if duree != 0 else 35\n",
    "            return round(montant * heures_travail * 4, 2)\n",
    "        elif isinstance(salaire, str) and 'Mensuel' in salaire:\n",
    "            montants = [float(m) for m in re.findall(r'\\d+\\.\\d+', salaire)]\n",
    "            if len(montants) == 2:\n",
    "                return round(max(montants), 2)\n",
    "            elif len(montants) == 1:\n",
    "                return round(montants[0], 2)\n",
    "        elif isinstance(salaire, str) and 'Annuel' in salaire:\n",
    "            montants_list = re.findall(r'\\d+\\.\\d+', salaire)\n",
    "            montant = float(montants_list[0]) if montants_list else 0\n",
    "            return round(montant / 12, 2)\n",
    "\n",
    "        return 0\n",
    "\n",
    "    # Appliquer la fonction d'extraction du salaire\n",
    "    df['salaire_libelle'] = df.apply(lambda row: extract_salaire(row['salaire_libelle'], row['dureeTravailLibelle']), axis=1)\n",
    "\n",
    "    # Calculer la moyenne des salaires\n",
    "    salaire_moyen = df['salaire_libelle'].mean()\n",
    "\n",
    "    # Remplacer les valeurs manquantes de salaire_libelle par la moyenne\n",
    "    df['salaire_libelle'].fillna(salaire_moyen, inplace=True)\n",
    "\n",
    "    # Transformer la colonne lieuTravail_libelle pour garder uniquement la partie après le tiret\n",
    "    df['lieuTravail_libelle'] = df['lieuTravail_libelle'].apply(lambda x: x.split(' - ')[1].strip() if ' - ' in x else x.strip())\n",
    "\n",
    "#     # Supprimer les colonnes non souhaitées\n",
    "#     df.drop(columns=['salaire_complement1', 'salaire_complement2', 'dateActualisation', 'appellationlibelle', 'dureeTravailLibelle', 'id'], inplace=True, errors='ignore')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Fonction de conversion des types de données\n",
    "def convert_data_types(df):\n",
    "    \"\"\"\n",
    "    Cette fonction convertit les types de données dans un DataFrame.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Le DataFrame à convertir.\n",
    "\n",
    "    Returns:\n",
    "    pd.DataFrame: Le DataFrame avec les types de données convertis.\n",
    "    \"\"\"\n",
    "    # Transformer la colonne dateCreation au format \"DD-MM-YYYY\"\n",
    "    df['dateCreation'] = pd.to_datetime(df['dateCreation'], errors='coerce').dt.strftime('%d-%m-%Y')\n",
    "\n",
    "    return df\n",
    "\n",
    "# Sauvegarde des données nettoyées dans un fichier CSV\n",
    "output_file = 'cleaned_offres_emploi.csv'\n",
    "\n",
    "def save_to_csv(df, output_file, first_chunk):\n",
    "    \"\"\"\n",
    "    Cette fonction sauvegarde un DataFrame dans un fichier CSV.\n",
    "\n",
    "    Args:\n",
    "    df (pd.DataFrame): Le DataFrame à sauvegarder.\n",
    "    output_file (str): Le chemin du fichier CSV.\n",
    "    first_chunk (bool): Indique s'il s'agit du premier morceau à sauvegarder.\n",
    "    \"\"\"\n",
    "    if first_chunk:\n",
    "        df.to_csv(output_file, index=False, mode='w', encoding='utf-8', sep=',')\n",
    "    else:\n",
    "        df.to_csv(output_file, index=False, mode='a', encoding='utf-8', sep=',', header=False)\n",
    "\n",
    "# Traitement des données par morceaux\n",
    "first_chunk = True\n",
    "total_documents_transformed = 0\n",
    "total_documents_stored = 0\n",
    "\n",
    "# Lire et traiter chaque morceau de données\n",
    "for chunk in read_large_json(file_path):\n",
    "    # Convertir le morceau en DataFrame\n",
    "    df_chunk = pd.DataFrame(chunk)\n",
    "\n",
    "    print(f\"Chunk de départ : {len(df_chunk)} documents\")\n",
    "    total_documents_transformed += len(df_chunk)\n",
    "    try:\n",
    "        # Nettoyer les données\n",
    "        df_chunk = clean_data(df_chunk)\n",
    "        # Convertir les types de données\n",
    "        df_chunk = convert_data_types(df_chunk)\n",
    "\n",
    "        # Renommer les colonnes\n",
    "        df_chunk.rename(columns={\n",
    "            'lieuTravail_libelle': 'Address',\n",
    "            'intitule': 'Title',\n",
    "            'description': 'Description',\n",
    "            'dateCreation': 'Date',\n",
    "            'typeContrat': 'Type of contract',\n",
    "            'salaire_libelle': 'Salary',\n",
    "            'origineOffre_urlOrigine': 'Link',\n",
    "            'entreprise_nom': 'Company'\n",
    "        }, inplace=True)\n",
    "\n",
    "        # Conserver uniquement les colonnes spécifiées et dans l'ordre souhaité\n",
    "        columns_to_keep = ['Title', 'Company', 'Description', 'Date', 'Address', 'Salary', 'Link']\n",
    "        df_chunk = df_chunk[columns_to_keep]\n",
    "\n",
    "        # Supprimer les doublons\n",
    "        df_chunk.drop_duplicates(inplace=True)\n",
    "\n",
    "        total_documents_stored += len(df_chunk)\n",
    "    except KeyError as e:\n",
    "        print(f\"Erreur KeyError : {e}\")\n",
    "        continue\n",
    "\n",
    "    # Sauvegarder les données nettoyées dans un fichier CSV\n",
    "    save_to_csv(df_chunk, output_file, first_chunk)\n",
    "    first_chunk = False\n",
    "    print(f\"Documents stockés après ce chunk : {total_documents_stored}\")\n",
    "\n",
    "print(f\"Nettoyage et sauvegarde des données terminés. Nombre total de documents transformés : {total_documents_transformed}\")\n",
    "print(f\"Nombre total de documents réellement stockés : {total_documents_stored}\")\n",
    "\n",
    "# Lire le fichier CSV et afficher le nombre de documents\n",
    "df_csv = pd.read_csv(output_file)\n",
    "print(f\"Nombre total de documents dans le fichier CSV : {len(df_csv)}\")\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "51f961e8",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Collecting ijson\n",
      "  Downloading ijson-3.2.3-cp310-cp310-win_amd64.whl.metadata (21 kB)\n",
      "Downloading ijson-3.2.3-cp310-cp310-win_amd64.whl (48 kB)\n",
      "   ---------------------------------------- 0.0/48.2 kB ? eta -:--:--\n",
      "   ---------------------------------------- 48.2/48.2 kB 2.5 MB/s eta 0:00:00\n",
      "Installing collected packages: ijson\n",
      "Successfully installed ijson-3.2.3\n"
     ]
    }
   ],
   "source": [
    "!pip3 install ijson"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "95e9c899",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3 (ipykernel)",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.9"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
